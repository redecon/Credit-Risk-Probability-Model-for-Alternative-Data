{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GITHUB_USER\"] = \"redecon\"\n",
        "os.environ[\"GITHUB_TOKEN\"] = \"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\"\n"
      ],
      "metadata": {
        "id": "fBbOlXDRMiiF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgJouGvQLIRv",
        "outputId": "3210c34e-1e4e-4321-e5a6-6a56f08588b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Credit-Risk-Probability-Model-for-Alternative-Data'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 30 (delta 8), reused 8 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (30/30), 200.75 KiB | 13.38 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "/content/Credit-Risk-Probability-Model-for-Alternative-Data\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/redecon/Credit-Risk-Probability-Model-for-Alternative-Data.git\n",
        "%cd Credit-Risk-Probability-Model-for-Alternative-Data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote set-url origin https://redecon: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@github.com/redecon/Credit-Risk-Probability-Model-for-Alternative-Data.git\n"
      ],
      "metadata": {
        "id": "OrAfNuTJM263"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"redietbekele02@outlook.com\"\n",
        "!git config --global user.name \"redecon\""
      ],
      "metadata": {
        "id": "K-u96zeEX0bj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# ============================================================\n",
        "# Custom Transformers\n",
        "# ============================================================\n",
        "\n",
        "class AggregateFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Creates aggregate features per customer.\"\"\"\n",
        "    def __init__(self, customer_id_col=\"CustomerId\", amount_col=\"Amount\"):\n",
        "        self.customer_id_col = customer_id_col\n",
        "        self.amount_col = amount_col\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        agg = (\n",
        "            X.groupby(self.customer_id_col)[self.amount_col]\n",
        "            .agg(\n",
        "                total_amount=\"sum\",\n",
        "                avg_amount=\"mean\",\n",
        "                txn_count=\"count\",\n",
        "                std_amount=\"std\",\n",
        "            )\n",
        "            .reset_index()\n",
        "        )\n",
        "        agg[\"std_amount\"] = agg[\"std_amount\"].fillna(0.0)\n",
        "        X = X.merge(agg, on=self.customer_id_col, how=\"left\")\n",
        "        return X\n",
        "\n",
        "\n",
        "class TemporalFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts temporal features from datetime column.\"\"\"\n",
        "    def __init__(self, datetime_col=\"TransactionStartTime\"):\n",
        "        self.datetime_col = datetime_col\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.datetime_col] = pd.to_datetime(X[self.datetime_col], errors=\"coerce\")\n",
        "        X[\"transaction_hour\"] = X[self.datetime_col].dt.hour.fillna(-1).astype(int)\n",
        "        X[\"transaction_day\"] = X[self.datetime_col].dt.day.fillna(-1).astype(int)\n",
        "        X[\"transaction_month\"] = X[self.datetime_col].dt.month.fillna(-1).astype(int)\n",
        "        X[\"transaction_year\"] = X[self.datetime_col].dt.year.fillna(-1).astype(int)\n",
        "        return X\n",
        "\n",
        "\n",
        "class WoETransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Weight of Evidence for selected categoricals. Requires y during fit.\"\"\"\n",
        "    def __init__(self, cat_cols=None, alpha=0.5):\n",
        "        self.cat_cols = cat_cols or []\n",
        "        self.alpha = alpha\n",
        "        self.woe_maps_ = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        df = X.copy()\n",
        "        df[\"target\"] = y\n",
        "\n",
        "        for col in self.cat_cols:\n",
        "            temp = df.groupby(col)[\"target\"].agg([\"sum\", \"count\"])\n",
        "            temp[\"good\"] = temp[\"count\"] - temp[\"sum\"]\n",
        "            temp[\"good\"] += self.alpha\n",
        "            temp[\"bad\"] = temp[\"sum\"] + self.alpha\n",
        "            total_good = temp[\"good\"].sum()\n",
        "            total_bad = temp[\"bad\"].sum()\n",
        "            temp[\"dist_good\"] = temp[\"good\"] / total_good\n",
        "            temp[\"dist_bad\"] = temp[\"bad\"] / total_bad\n",
        "            temp[\"woe\"] = np.log(temp[\"dist_good\"] / temp[\"dist_bad\"])\n",
        "            self.woe_maps_[col] = temp[\"woe\"].to_dict()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.cat_cols:\n",
        "            woe_col = f\"{col}_woe\"\n",
        "            X[woe_col] = X[col].map(self.woe_maps_[col]).fillna(0.0)\n",
        "        return X\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Pipeline Builder\n",
        "# ============================================================\n",
        "\n",
        "def build_preprocessing_pipeline(\n",
        "    numeric_cols,\n",
        "    categorical_cols,\n",
        "    woe_categorical_cols=None,\n",
        "    customer_id_col=\"CustomerId\",\n",
        "    amount_col=\"Amount\",\n",
        "    datetime_col=\"TransactionStartTime\",\n",
        "    target_col=\"FraudResult\",  # Updated to match your data\n",
        "    use_woe=False,\n",
        "):\n",
        "    woe_categorical_cols = woe_categorical_cols or []\n",
        "\n",
        "    base_steps = [\n",
        "        (\"agg_features\", AggregateFeaturesTransformer(customer_id_col, amount_col)),\n",
        "        (\"temporal_features\", TemporalFeaturesTransformer(datetime_col)),\n",
        "    ]\n",
        "\n",
        "    if use_woe:\n",
        "        base_steps.append((\"woe\", WoETransformer(cat_cols=woe_categorical_cols)))\n",
        "\n",
        "    # After base transforms, dynamically select columns\n",
        "    class DynamicPreprocessor(BaseEstimator, TransformerMixin):\n",
        "        def __init__(self):\n",
        "            self.numeric_cols_ = numeric_cols + [c + \"_woe\" for c in woe_categorical_cols if use_woe]\n",
        "            self.categorical_cols_ = [c for c in categorical_cols if c not in woe_categorical_cols]\n",
        "\n",
        "        def fit(self, X, y=None):\n",
        "            transformers = []\n",
        "\n",
        "            if self.numeric_cols_:\n",
        "                num_pipe = Pipeline([\n",
        "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                    (\"scaler\", StandardScaler()),\n",
        "                ])\n",
        "                transformers.append((\"num\", num_pipe, self.numeric_cols_))\n",
        "\n",
        "            if self.categorical_cols_:\n",
        "                cat_pipe = Pipeline([\n",
        "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "                ])\n",
        "                transformers.append((\"cat\", cat_pipe, self.categorical_cols_))\n",
        "\n",
        "            self.preprocessor_ = ColumnTransformer(\n",
        "                transformers=transformers,\n",
        "                remainder=\"drop\"\n",
        "            )\n",
        "            self.preprocessor_.fit(X)\n",
        "            return self\n",
        "\n",
        "        def transform(self, X):\n",
        "            if not hasattr(self, 'preprocessor_'):\n",
        "                raise RuntimeError(\"Must fit first!\")\n",
        "            return self.preprocessor_.transform(X)\n",
        "\n",
        "    full_pipeline = Pipeline([\n",
        "        (\"base\", Pipeline(base_steps)),\n",
        "        (\"dynamic_preprocess\", DynamicPreprocessor()),\n",
        "    ])\n",
        "\n",
        "    return full_pipeline\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Usage - Adjusted for your actual data\n",
        "# ============================================================\n",
        "\n",
        "# Load your data (update the path if needed)\n",
        "df_raw = pd.read_csv(\"/content/data.csv\")  # or the actual filename\n",
        "\n",
        "print(\"Columns:\", df_raw.columns.tolist())\n",
        "print(\"Shape:\", df_raw.shape)\n",
        "\n",
        "# Define columns based on your data\n",
        "numeric_cols = [\n",
        "    \"Amount\", \"Value\",\n",
        "    \"total_amount\", \"avg_amount\", \"txn_count\", \"std_amount\",\n",
        "    \"transaction_hour\", \"transaction_day\", \"transaction_month\", \"transaction_year\"\n",
        "]\n",
        "\n",
        "categorical_cols = [\"ProductCategory\", \"ChannelId\", \"ProviderId\", \"PricingStrategy\"]\n",
        "\n",
        "woe_categorical_cols = [\"ProductCategory\", \"ChannelId\"]  # Good choices\n",
        "\n",
        "# Target column is FraudResult (0/1)\n",
        "has_target = \"FraudResult\" in df_raw.columns\n",
        "\n",
        "pipeline = build_preprocessing_pipeline(\n",
        "    numeric_cols=numeric_cols,\n",
        "    categorical_cols=categorical_cols,\n",
        "    woe_categorical_cols=woe_categorical_cols,\n",
        "    customer_id_col=\"CustomerId\",\n",
        "    amount_col=\"Amount\",\n",
        "    datetime_col=\"TransactionStartTime\",\n",
        "    use_woe=has_target  # Only apply WoE if target exists (train data)\n",
        ")\n",
        "\n",
        "if has_target:\n",
        "    X_processed = pipeline.fit_transform(df_raw, df_raw[\"FraudResult\"])\n",
        "else:\n",
        "    X_processed = pipeline.fit_transform(df_raw)\n",
        "\n",
        "print(\"Processed shape:\", X_processed.shape)\n",
        "\n",
        "# Save (customer-level features)\n",
        "pd.DataFrame(X_processed).to_csv(\"/content/processed_task3.csv\", index=False)\n",
        "print(\"Saved to /content/processed_task3.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZzI7X6VNOgW",
        "outputId": "da06ca28-bb37-4a9e-f2dc-9423a24f699f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'Amount', 'Value', 'TransactionStartTime', 'PricingStrategy', 'FraudResult']\n",
            "Shape: (95662, 16)\n",
            "Processed shape: (95662, 22)\n",
            "Saved to /content/processed_task3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Stage all your changes (new files, modified files, etc.)\n",
        "!git add .\n",
        "\n",
        "# 2. Commit your work for Task 3\n",
        "!git commit -m \"feat: complete task 3 - robust feature engineering pipeline with aggregates, temporal features, WoE, and sklearn Pipeline\"\n",
        "\n",
        "# 3. Make sure main is up to date (safe to do)\n",
        "!git pull origin main\n",
        "\n",
        "# 4. Create and switch to a new branch called task-3\n",
        "!git checkout -b task-3\n",
        "\n",
        "# 5. Push the new task-3 branch to GitHub\n",
        "!git push origin task-3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh2IiZP1XCwD",
        "outputId": "c9ca4a93-dd17-4466-d0c9-85eb64f63743"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch task-3\n",
            "nothing to commit, working tree clean\n",
            "From https://github.com/redecon/Credit-Risk-Probability-Model-for-Alternative-Data\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "fatal: A branch named 'task-3' already exists.\n",
            "Total 0 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "remote: \n",
            "remote: Create a pull request for 'task-3' on GitHub by visiting:\u001b[K\n",
            "remote:      https://github.com/redecon/Credit-Risk-Probability-Model-for-Alternative-Data/pull/new/task-3\u001b[K\n",
            "remote: \n",
            "To https://github.com/redecon/Credit-Risk-Probability-Model-for-Alternative-Data.git\n",
            " * [new branch]      task-3 -> task-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9vdx1Ovgftnc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}