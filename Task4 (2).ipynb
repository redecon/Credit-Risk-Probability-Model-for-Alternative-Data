{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Proxy target variable engineering with RFM and clustering"
      ],
      "metadata": {
        "id": "b7Yb937y5jfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute RFM metrics per customer"
      ],
      "metadata": {
        "id": "YBT8kW-r5tTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Load raw transactions and ensure datetime ---\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Adjust path as needed\n",
        "df_raw = pd.read_csv(\"/content/data.csv\")\n",
        "\n",
        "# Robust datetime parsing\n",
        "df_raw[\"TransactionStartTime\"] = pd.to_datetime(\n",
        "    df_raw[\"TransactionStartTime\"], errors=\"coerce\"\n",
        ")\n",
        "\n",
        "# Drop rows without a valid datetime or CustomerId/Amount for RFM integrity\n",
        "df_raw = df_raw.dropna(subset=[\"TransactionStartTime\", \"CustomerId\", \"Amount\"])\n",
        "\n",
        "# --- Step 2: Snapshot date for consistent Recency ---\n",
        "snapshot_date = df_raw[\"TransactionStartTime\"].max()\n",
        "if pd.isna(snapshot_date):\n",
        "    raise ValueError(\"No valid TransactionStartTime values found to compute RFM.\")\n",
        "snapshot_date = snapshot_date + pd.Timedelta(days=1)\n",
        "\n",
        "# --- Step 3: RFM aggregation (one row per CustomerId) ---\n",
        "rfm = df_raw.groupby(\"CustomerId\").agg(\n",
        "    Recency=(\"TransactionStartTime\", lambda x: (snapshot_date - x.max()).days),\n",
        "    Frequency=(\"TransactionStartTime\", \"count\"),\n",
        "    Monetary=(\"Amount\", \"sum\"),\n",
        ").reset_index()\n",
        "\n",
        "# Guard against missing/invalid numeric values\n",
        "rfm[\"Recency\"] = pd.to_numeric(rfm[\"Recency\"], errors=\"coerce\").fillna(rfm[\"Recency\"].median())\n",
        "rfm[\"Frequency\"] = pd.to_numeric(rfm[\"Frequency\"], errors=\"coerce\").fillna(0)\n",
        "rfm[\"Monetary\"] = pd.to_numeric(rfm[\"Monetary\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "print(\"RFM head:\\n\", rfm.head())\n",
        "print(\"RFM shape:\", rfm.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO0LiPEc5pQh",
        "outputId": "c89a51d7-afcd-409e-d0a4-f03df0df074c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFM head:\n",
            "         CustomerId  Recency  Frequency  Monetary\n",
            "0     CustomerId_1       35          1  -10000.0\n",
            "1    CustomerId_10       35          1  -10000.0\n",
            "2  CustomerId_1001       41          5   20000.0\n",
            "3  CustomerId_1002       16          8    3325.0\n",
            "4  CustomerId_1004        4          1    2000.0\n",
            "RFM shape: (1952, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute RFM metrics per customer"
      ],
      "metadata": {
        "id": "T8lQVbRS6LLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Scale RFM features ---\n",
        "rfm_features = rfm[[\"Recency\", \"Frequency\", \"Monetary\"]].copy()\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm_features)\n",
        "\n",
        "# --- Step 5: K-Means clustering (3 clusters, reproducible) ---\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=\"auto\")\n",
        "rfm[\"Cluster\"] = kmeans.fit_predict(rfm_scaled)\n",
        "\n",
        "# --- Step 6: Cluster profiling to identify high-risk segment ---\n",
        "cluster_summary = (\n",
        "    rfm.groupby(\"Cluster\")[[\"Recency\", \"Frequency\", \"Monetary\"]].mean().sort_index()\n",
        ")\n",
        "print(\"Cluster summary (means):\\n\", cluster_summary)\n",
        "\n",
        "# Define high-risk as least engaged: low Frequency and low Monetary (tie-break by high Recency)\n",
        "ranked = cluster_summary.assign(\n",
        "    freq_rank=cluster_summary[\"Frequency\"].rank(method=\"min\", ascending=True),\n",
        "    mon_rank=cluster_summary[\"Monetary\"].rank(method=\"min\", ascending=True),\n",
        "    rec_rank=cluster_summary[\"Recency\"].rank(method=\"min\", ascending=False),\n",
        ")\n",
        "ranked[\"risk_score\"] = ranked[\"freq_rank\"] + ranked[\"mon_rank\"] + ranked[\"rec_rank\"]\n",
        "high_risk_cluster = ranked[\"risk_score\"].idxmin()\n",
        "\n",
        "# Assign binary target\n",
        "rfm[\"is_high_risk\"] = (rfm[\"Cluster\"] == high_risk_cluster).astype(int)\n",
        "print(\"Chosen high-risk cluster:\", high_risk_cluster)\n",
        "print(\"Target distribution:\", rfm[\"is_high_risk\"].value_counts().to_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNwGK1Da6GtW",
        "outputId": "bec8dca0-6cab-422c-a899-fe52d0f5a045"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster summary (means):\n",
            "            Recency    Frequency      Monetary\n",
            "Cluster                                      \n",
            "0         6.519969    26.672670  1.883884e+05\n",
            "1        13.000000  2075.000000 -5.328000e+07\n",
            "2        29.348665     7.059347  5.899985e+04\n",
            "Chosen high-risk cluster: 2\n",
            "Target distribution: {0: 1278, 1: 674}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge target back into Task 3 features and save"
      ],
      "metadata": {
        "id": "QH5T36SE6XKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 7: Load Task 3 processed features ---\n",
        "processed_df = pd.read_csv(\"/content/processed_task3 (2).csv\")\n",
        "\n",
        "# Handle passthrough naming from ColumnTransformer (remainder__CustomerId)\n",
        "if \"CustomerId\" not in processed_df.columns:\n",
        "    if \"remainder__CustomerId\" in processed_df.columns:\n",
        "        processed_df = processed_df.rename(columns={\"remainder__CustomerId\": \"CustomerId\"})\n",
        "    else:\n",
        "        raise KeyError(\"CustomerId column not found in processed_task3.csv. Ensure Task 3 preserved IDs.\")\n",
        "\n",
        "# Ensure CustomerId types align for merging\n",
        "rfm[\"CustomerId\"] = pd.to_numeric(rfm[\"CustomerId\"], errors=\"coerce\")\n",
        "processed_df[\"CustomerId\"] = pd.to_numeric(processed_df[\"CustomerId\"], errors=\"coerce\")\n",
        "\n",
        "# Drop any rows with missing IDs before merge\n",
        "rfm = rfm.dropna(subset=[\"CustomerId\"])\n",
        "processed_df = processed_df.dropna(subset=[\"CustomerId\"])\n",
        "\n",
        "# --- Step 8: Merge proxy target into processed features ---\n",
        "final_df = processed_df.merge(\n",
        "    rfm[[\"CustomerId\", \"is_high_risk\"]],\n",
        "    on=\"CustomerId\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Sanity checks\n",
        "print(\"is_high_risk NaNs after merge:\", int(final_df[\"is_high_risk\"].isna().sum()))\n",
        "print(\"Rows before dropping NaNs:\", final_df.shape[0])\n",
        "\n",
        "# Drop rows without target to ensure model-ready data\n",
        "final_df = final_df.dropna(subset=[\"is_high_risk\"])\n",
        "\n",
        "# Convert to integer type explicitly\n",
        "final_df[\"is_high_risk\"] = final_df[\"is_high_risk\"].astype(int)\n",
        "\n",
        "print(\"Rows after dropping NaNs:\", final_df.shape[0])\n",
        "print(\"Final target distribution:\", final_df[\"is_high_risk\"].value_counts().to_dict())\n",
        "\n",
        "# --- Step 9: Save Task 4 dataset ---\n",
        "final_df.to_csv(\"/content/processed_task4.csv\", index=False)\n",
        "print(\"Saved /content/processed_task4.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkoJr2jo6Wkd",
        "outputId": "49b8e521-7b16-4fff-a843-4848588b966e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_high_risk NaNs after merge: 0\n",
            "Rows before dropping NaNs: 0\n",
            "Rows after dropping NaNs: 0\n",
            "Final target distribution: {}\n",
            "Saved /content/processed_task4.csv\n"
          ]
        }
      ]
    }
  ]
}